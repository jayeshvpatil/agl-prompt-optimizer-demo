"""
Summary of the Agent-Lightning Movie Selector Demo
===================================================

What we've built:
âœ… A complete agent-lightning demo from scratch
âœ… Quick demo for instant gratification
âœ… Multi-task demo to show versatility  
âœ… Real iterative prompt optimization (custom loop)
âœ… Comprehensive documentation

File Structure:
===============

Core Files:
-----------
1. movie_selector_agent.py
   - Main agent logic with @agl.rollout decorator
   - Task definition (MovieSelectionTask)
   - Tool implementation (get_available_movies)
   - Grader function (grade_movie_selection)
   - Perfect for understanding agent patterns

2. demo.py
   - Single task demonstration
   - Shows each step: LLM â†’ Tool â†’ Decision â†’ Reward
   - Great for first-time viewers
   - Run: uv run demo.py

3. demo_multi_task.py
   - Multiple scenarios in one run
   - Shows performance summary
   - Demonstrates versatility
   - Run: uv run demo_multi_task.py

4. prompt_optimization_training.py
   - Custom iterative prompt optimization (evaluate â†’ critique â†’ rewrite â†’ re-evaluate)
   - Tracks accuracy, average reward, confusion map over rounds
   - Saves artifacts: prompt_optimization_results.json & training_results.md
   - Run: uv run prompt_optimization_training.py

Documentation (Lean):
---------------------
1. README.md
   - Quick setup
   - How to run demos & optimization
   - Minimal customization tips

How to Use:
===========

For Quick Team Demo (5 min):
---------------------------
1. Show demo.py output:
   - Single task
   - Perfect reward (1.0)
   - Agent's decision process

2. Or run demo_multi_task.py:
   - Three tasks
   - 100% accuracy
   - Professional summary

For Technical Deep Dive (30 min):
---------------------------------
1. Run quick demo
2. Show movie_selector_agent.py code
3. Explain:
   - Task definition
   - Agent logic with tools
   - Rollout concept
   - Reward calculation
4. Discuss optimization potential

For Prompt Optimization (10+ min):
---------------------------------
1. Run demo.py to see a single baseline behavior
2. Run prompt_optimization_training.py for multi-round improvement
3. Observe output:
   - Baseline accuracy vs per-round deltas
   - Critique text the model generates each round
   - Confusion analysis (persistent mis-selections)
   - Regression or plateau detection

Key Concepts to Explain:
======================

1. Task
   - Input specification for the agent
   - Example: "3 people, action+comedy, <2h"

2. Rollout
   - One complete execution
   - Agent receives task â†’ uses tools â†’ gets reward

3. Spans
   - Individual operations within rollout
   - LLM call, tool execution, grading

4. Prompt Template
   - Instructions given to LLM
   - Our custom loop critiques & rewrites this between rounds

5. Reward
   - Score for agent's decision
   - 1.0 = perfect, 0.0 = wrong

The Agent Flow:
==============

â”Œâ”€ User Task â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ 3 people, action + comedy, <120min  â”‚
â””â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
      â”‚
â”Œâ”€â”€â”€â”€â”€â–¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ LLM Call #1 (with prompt template)  â”‚
â”‚ "I need to check available movies"  â”‚
â””â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
      â”‚
â”Œâ”€â”€â”€â”€â”€â–¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ Tool Execution: get_available_moviesâ”‚
â”‚ Returns: [Action Blast, Laugh Riot] â”‚
â””â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
      â”‚
â”Œâ”€â”€â”€â”€â”€â–¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ LLM Call #2 (with tool results)     â”‚
â”‚ "Based on ratings, recommend ..."   â”‚
â””â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
      â”‚
â”Œâ”€â”€â”€â”€â”€â–¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ Reward Calculation                  â”‚
â”‚ Expected: Action Blast              â”‚
â”‚ Got: Action Blast â†’ Reward: 1.0 âœ…  â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜

What Makes This Cool:
====================

1. Real AI Agent Pattern
   - Tool use (function calling)
   - Multi-turn reasoning
   - Reward-based evaluation

2. Agent-Lightning Features
   - @agl.rollout decorator for tracking
   - Automatic prompt optimization
   - Parallel evaluation
   - Structured training loop

3. Professional Quality
   - Clean code structure
   - Comprehensive documentation
   - Multiple demo options
   - Easy customization

Potential Improvements:
======================

1. Add more movies to database
2. Add new genres/constraints
3. Create harder test cases
4. Run full APO training
5. Compare baseline vs optimized prompts
6. Track metrics over time
7. Implement caching for cheaper runs
8. Add streaming output for real-time feedback

Real-World Applications:
========================

Same pattern works for:
- Customer support routing
- SQL query generation
- Code generation and review
- Data extraction from documents
- Recommendation systems
- Content summarization
- Information retrieval

How to Customize:
=================

Change Movies:
  Edit MOVIES_DATABASE in movie_selector_agent.py

Add New Genres:
  Add to movies and update prompts

Make Harder:
  Add more specific requirements to tasks
  Reduce max_duration
  Require exact matches (not just genre)

Optimize Faster:
   Lower rounds in prompt_optimization_training.py for quick iteration
   Use cheaper model (gpt-4o-mini or gpt-3.5-turbo)
   Subset tasks to fastest failing cases

Run Comparison:
  Save baseline rewards
  Run training
  Compare improvements

Next Steps for Your Team:
=========================

1. âœ… Run demo.py
2. âœ… Show demo_multi_task.py results
3. âœ… Explain concepts using movie_selector_agent.py (core logic) and README.md
4. â­ Run prompt_optimization_training.py to show iterative improvement
5. ðŸŽ¯ Adapt pattern to your own problems
6. ðŸ“Š Track improvements over time

Commands Reference:
===================

# Quick test (fast, visual)
uv run demo.py

# Multiple scenarios (professional)
uv run demo_multi_task.py

# Prompt optimization (multi-round)
uv run prompt_optimization_training.py

# Development/debugging
python -m pdb movie_selector_agent.py

Environment Setup:
==================

Requirements:
- Python 3.12+
- OpenAI API key
- uv package manager

Setup:
echo "OPENAI_API_KEY=sk-proj-..." > .env
uv sync
uv run demo.py

Cost Estimation:
================

Single demo.py run: ~$0.01-0.02
Multi-task demo: ~$0.02-0.05
Full training (10 min): ~$1-2

Tips:
- Use gpt-4o-mini to keep costs low
- Reuse responses during development
- Cache results for repeated testing

"""

if __name__ == "__main__":
    print(__doc__)
